{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt, animation, patches\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "\n",
    "import ephem\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from skyfield import almanac\n",
    "from skyfield.api import load\n",
    "from shapely.geometry import LineString, Polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/laytonrosenfeld/Desktop/Evolved/ADP electricity sample.xlsx\"\n",
    "lat_long_file = \"/Users/laytonrosenfeld/Desktop/Evolved/zone_lat_lon.csv\"\n",
    "shapefile_path = \"/Users/laytonrosenfeld/Desktop/Evolved/adp_regions/adp_regions.shp\"\n",
    "save_path = \"/Users/laytonrosenfeld/Desktop/Evolved/animations\"\n",
    "\n",
    "interpolate_minutes = 15\n",
    "month = 7\n",
    "day = 27\n",
    "year = 2011\n",
    "date = f'{year}-{month:02d}-{day:02d}'\n",
    "frames_per_second=2\n",
    "scenario = 'central'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_data_to = pd.read_excel(input_file, sheet_name=0)\n",
    "adp_data_from = pd.read_excel(input_file, sheet_name=1)\n",
    "lat_long_info = pd.read_csv(lat_long_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission = adp_data_to[adp_data_to['From||Type'] == 'transmission']\n",
    "\n",
    "def extract_zones(row):\n",
    "    locations = row['From||Outputs Group Aggregate'].split('||')\n",
    "    destination = row['Zone']\n",
    "    source = next(loc for loc in locations if loc != destination)\n",
    "    return pd.Series([source, destination])\n",
    "\n",
    "\n",
    "def fill_missing_hours(transmission):\n",
    "    df = transmission.copy()\n",
    "    df['date'] = df['Weather Datetime'].dt.date  # Extract date for grouping\n",
    "    df = df.set_index('Weather Datetime')  # Set index for resampling\n",
    "\n",
    "    filled_results = []  \n",
    "\n",
    "    for (source, destination, date), group in df.groupby(['Source', 'Destination', 'date']):\n",
    "        full_range = pd.date_range(start=date, periods=24, freq='h')  # Generate all 24 hours\n",
    "        group_resampled = group.reindex(full_range).fillna({'Value': 0})  # Fill missing hours with 0\n",
    "        group_resampled[['Source', 'Destination']] = source, destination  # Preserve categorical data\n",
    "        group_resampled = group_resampled.reset_index().rename(columns={'index': 'Weather Datetime'})\n",
    "        filled_results.append(group_resampled)\n",
    "\n",
    "    filled_df = pd.concat(filled_results, ignore_index=True)\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_transmission(transmission):\n",
    "    df = transmission.copy()  # Avoid modifying the original DataFrame\n",
    "    df['date'] = df['Weather Datetime'].dt.date  # Extract date (day-level grouping)\n",
    "    df = df.set_index('Weather Datetime')  # Set index for resampling\n",
    "\n",
    "    interpolated_results = []  # List to store results\n",
    "\n",
    "    for (source, destination, date), group in df.groupby(['Source', 'Destination', 'date']):\n",
    "        group_resampled = group.resample('15min').interpolate(method='linear')  # Interpolate numeric columns\n",
    "        group_resampled = group_resampled.ffill()  # Forward fill non-numeric columns\n",
    "        group_resampled = group_resampled.reset_index()  # Reset index for merging later\n",
    "                \n",
    "        interpolated_results.append(group_resampled)  # Store result\n",
    "\n",
    "    interpolated_df = pd.concat(interpolated_results, ignore_index=True)\n",
    "    return interpolated_df\n",
    "\n",
    "def compute_net_flows(transmission):\n",
    "    net_flow_data = []\n",
    "    transmission['Quarter_Hour'] = transmission['Weather Datetime'].dt.floor('15min')  \n",
    "\n",
    "    grouped = transmission.groupby(['Run Name', 'Year', 'Quarter_Hour'])  # Group by 15-minute intervals\n",
    "\n",
    "    for (run_name, year, quarter_hour), group in grouped:\n",
    "        # Create lookup dictionary for flows\n",
    "        flow_dict = dict(zip(zip(group['Source'], group['Destination']), group['Value']))\n",
    "\n",
    "        processed_pairs = set()\n",
    "\n",
    "        for (zone1, zone2), value1 in flow_dict.items():\n",
    "            if (zone2, zone1) in flow_dict:\n",
    "                if (zone1, zone2) not in processed_pairs:\n",
    "                    value2 = flow_dict[(zone2, zone1)]\n",
    "                    if value1 > value2:\n",
    "                        net_flow_data.append((zone1, zone2, run_name, year, quarter_hour, value1 - value2))\n",
    "                    elif value2 > value1:\n",
    "                        net_flow_data.append((zone2, zone1, run_name, year, quarter_hour, value2 - value1))\n",
    "                    processed_pairs.add((zone1, zone2))\n",
    "                    processed_pairs.add((zone2, zone1))\n",
    "            elif (zone1, zone2) not in processed_pairs:\n",
    "                net_flow_data.append((zone1, zone2, run_name, year, quarter_hour, value1))\n",
    "                processed_pairs.add((zone1, zone2))\n",
    "\n",
    "    net_flow_df = pd.DataFrame(net_flow_data, columns=['Source', 'Destination', 'Run Name', 'Year', 'Weather Datetime', 'Value'])\n",
    "    return net_flow_df\n",
    "\n",
    "\n",
    "\n",
    "transmission[['Source', 'Destination']] = transmission.apply(extract_zones, axis=1)\n",
    "\n",
    "transmission_filled = fill_missing_hours(transmission)\n",
    "\n",
    "transmission_interpolated = interpolate_transmission(transmission_filled)\n",
    "\n",
    "net_transmission = compute_net_flows(transmission_interpolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_adp(adp_data, type_col_labels):\n",
    "    df = adp_data.copy()  # Avoid modifying the original DataFrame\n",
    "    df['Weather Datetime'] = pd.to_datetime(df['Weather Datetime'])  # Ensure datetime format\n",
    "    df['date'] = df['Weather Datetime'].dt.date  # Extract date (day-level grouping)\n",
    "    \n",
    "    # Create the combination of columns to check for duplicates\n",
    "    group_columns = ['Weather Datetime', 'Zone', type_col_labels] if isinstance(type_col_labels, list) else ['Weather Datetime', 'Zone', type_col_labels]\n",
    "    \n",
    "    # Check for duplicates based on the combination of 'Weather Datetime', 'Zone', and type_col_labels\n",
    "    duplicate_timestamps = df[df.duplicated(subset=group_columns, keep=False)]\n",
    "    \n",
    "    df = df.set_index('Weather Datetime')  # Set index for resampling\n",
    "\n",
    "    interpolated_results = []  # List to store results\n",
    "\n",
    "    for zone, group in df.groupby(['Zone', 'date', type_col_labels]): \n",
    "        # Drop duplicates within the group (if any)\n",
    "        group = group[~group.index.duplicated(keep='first')]\n",
    "\n",
    "        group = group.infer_objects(copy=False)\n",
    "        \n",
    "        group_resampled = group.resample(f'{interpolate_minutes}min').interpolate(method='linear')  # Interpolate numeric columns\n",
    "        \n",
    "        # Forward-fill non-numeric columns\n",
    "        for col in group.select_dtypes(exclude=['number']).columns:\n",
    "            group_resampled[col] = group_resampled[col].ffill()\n",
    "        \n",
    "        group_resampled = group_resampled.reset_index()  # Reset index for merging later\n",
    "        interpolated_results.append(group_resampled)  # Store result\n",
    "\n",
    "    interpolated_df = pd.concat(interpolated_results, ignore_index=True)\n",
    "    return interpolated_df\n",
    "\n",
    "\n",
    "adp_to_filtered = adp_data_to[adp_data_to[\"From||Type\"] != 'transmission'].fillna({'Value': 0})\n",
    "adp_from_filtered = adp_data_from[adp_data_from[\"To||Type\"] != 'transmission'].fillna({'Value': 0})\n",
    "\n",
    "adp_to_interp = interpolate_adp(adp_to_filtered, \"From||Outputs Group Aggregate\")\n",
    "adp_from_interp = interpolate_adp(adp_from_filtered, \"To||Outputs Group Aggregate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_color = \"#4682b4\"\n",
    "pie_multiplier = 1/200\n",
    "\n",
    "color_mapping_to = {\n",
    "    'electricity storage': '#9b59b6',  # Soft purple\n",
    "    'gas power': '#d2691e',  # Soft brown\n",
    "    'onshore wind': '#2e8b57',  # Soft green\n",
    "    'solar': '#f4c542',  # Soft yellow\n",
    "    'canadian imports': '#4682b4',  # Soft blue\n",
    "    'gas power w/cc': '#ff8c00',  # Soft orange\n",
    "    'geothermal power': '#87cefa',  # Light blue\n",
    "    'hydro': '#00bcd4',  # Soft cyan\n",
    "    'nuclear power': '#808080',  # Soft grey\n",
    "    'offshore wind': '#1e3a8a',  # Dark blue\n",
    "    'oil power': '#2f2f2f'  # Dark grey (almost black)\n",
    "}\n",
    "\n",
    "color_mapping_elec_use = {\n",
    "    'boiler': '#1f77b4',  # Blue\n",
    "    'commercial': '#ff7f0e',  # Orange\n",
    "    'data center': '#2ca02c',  # Green\n",
    "    'electricity storage': '#d62728',  # Red\n",
    "    'electrolysis h2': '#9467bd',  # Purple\n",
    "    'productive': '#8c564b',  # Brown\n",
    "    'residential': '#e377c2',  # Pink\n",
    "    'thermal energy storage': '#7f7f7f',  # Gray\n",
    "    'transportation': '#bcbd22',  # Olive\n",
    "    'upstream losses': '#17becf',  # Cyan\n",
    "    'biofuels': '#aec7e8',  # Light Blue\n",
    "    'haber-bosch': '#ffbb78',  # Light Orange\n",
    "    'hydrogen liquefaction': '#98df8a',  # Light Green\n",
    "    'iron and steel production': '#ff9896',  # Light Red\n",
    "    'lng production': '#c5b0d5'  # Light Purple\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_terminator_coordinates(input_datetime, nlons=360):\n",
    "    \"\"\"\n",
    "    Compute day-night terminator coordinates for a given datetime.\n",
    "\n",
    "    Args:\n",
    "        input_datetime (str or datetime): The date and time in UTC (e.g., '2011-01-29 13:00:00').\n",
    "        nlons (int): Number of longitudes to compute (default is 360).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: DataFrame with columns ['Longitude', 'Latitude'] containing terminator coordinates,\n",
    "                              or None if no coordinates are within the USA latitude range.\n",
    "    \"\"\"\n",
    "    dg2rad = np.pi / 180.0\n",
    "    est = timezone(timedelta(hours=-5))\n",
    "    \n",
    "    # Convert input to UTC datetime\n",
    "    if isinstance(input_datetime, str):\n",
    "        input_datetime = datetime.strptime(input_datetime, '%Y-%m-%d %H:%M:%S')\n",
    "    date_est = input_datetime.replace(tzinfo=est)  \n",
    "    date_utc = date_est.astimezone(timezone.utc)  \n",
    "\n",
    "    # Set up the observer\n",
    "    observer = ephem.Observer()\n",
    "    observer.date = ephem.Date(date_utc)\n",
    "\n",
    "    # Compute solar declination and hour angle\n",
    "    sun = ephem.Sun(observer)\n",
    "    sun.compute(observer)\n",
    "\n",
    "    dec = np.degrees(sun.dec)  # Solar declination in degrees\n",
    "    tau = np.degrees(observer.sidereal_time() - sun.ra)  # Hour angle in degrees\n",
    "\n",
    "    # Generate longitudes for the continental USA\n",
    "    lons = np.linspace(-125, -67.5, nlons)\n",
    "    longitude = lons + tau  # Adjust longitudes based on hour angle\n",
    "\n",
    "    # Compute latitudes\n",
    "    lats = np.arctan(-np.cos(longitude * dg2rad) / np.tan(dec * dg2rad)) / dg2rad\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'Longitude': lons, 'Latitude': lats})\n",
    "\n",
    "    # Filter latitudes to within the continental USA range (24.5° to 49.5°)\n",
    "    df = df[(df['Latitude'] >= 24.5) & (df['Latitude'] <= 49.5)]\n",
    "\n",
    "    # Return None if no valid coordinates exist\n",
    "    return df if not df.empty else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def energy_animation(date, shapefile_path, transmission, adp_interp, lat_long_info, color_mapping, pie_multiplier, arrow_color, frames_per_second, save_path, chart_type, interpolate_minutes, scenario):\n",
    "    # Load US map and filter for the continental US\n",
    "    interval_ms = 100\n",
    "    us_map = gpd.read_file(shapefile_path)\n",
    "    us_map = us_map.to_crs(epsg=4326)\n",
    "\n",
    "    # Set up figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    us_map.plot(ax=ax, color='white', edgecolor='black')\n",
    "\n",
    "    # Global storage for arrows and pies\n",
    "    arrow_patches = []\n",
    "    pie_axes = []\n",
    "\n",
    "    if chart_type not in [\"to\", \"from\"]:\n",
    "        raise ValueError(\"chart_type must be either 'to' or 'from'\")\n",
    "\n",
    "    type_col_labels = \"From||Outputs Group Aggregate\" if chart_type == \"to\" else \"To||Outputs Group Aggregate\"\n",
    "\n",
    "    adp_interp = adp_interp[adp_interp['Run Name'] == scenario]\n",
    "    transmission = transmission[transmission['Run Name'] == scenario]\n",
    "\n",
    "    def update(frame):\n",
    "        nonlocal arrow_patches, pie_axes\n",
    "        global terminator_line  # Declare a variable to track the terminator line\n",
    "\n",
    "        # Clear previous arrows and pie charts\n",
    "        for arrow in arrow_patches:\n",
    "            arrow.remove()\n",
    "        for pie_ax in pie_axes:\n",
    "            pie_ax.remove()\n",
    "\n",
    "        arrow_patches.clear()\n",
    "        pie_axes.clear()\n",
    "\n",
    "        if \"terminator_line\" in globals() and terminator_line is not None:\n",
    "            terminator_line.remove()\n",
    "            terminator_line = None\n",
    "\n",
    "        # Define input datetime for current frame\n",
    "        input_datetime = pd.Timestamp(date) + pd.Timedelta(minutes=interpolate_minutes * frame)\n",
    "\n",
    "        # Filter transmission data\n",
    "        filtered_transmission = transmission[transmission['Weather Datetime'] == input_datetime].dropna(subset=['Value'])\n",
    "\n",
    "        # Compute pie chart sizes\n",
    "        pie_sizes = {}\n",
    "        filtered_adp = adp_interp[adp_interp['Weather Datetime'] == input_datetime]\n",
    "        grouped_by_zone = filtered_adp.groupby('Zone')\n",
    "\n",
    "        for zone, group in grouped_by_zone:\n",
    "            total_value = group['Value'].sum()\n",
    "            pie_sizes[zone] = total_value * pie_multiplier\n",
    "\n",
    "        # Draw transmission arrows\n",
    "        for _, row in filtered_transmission.iterrows():\n",
    "            source = lat_long_info[lat_long_info['zone'] == row['Source']]\n",
    "            dest = lat_long_info[lat_long_info['zone'] == row['Destination']]\n",
    "\n",
    "            if not source.empty and not dest.empty:\n",
    "                full_line = LineString([(source.iloc[0]['longitude'], source.iloc[0]['latitude']),\n",
    "                                        (dest.iloc[0]['longitude'], dest.iloc[0]['latitude'])])\n",
    "\n",
    "                buffer_start = pie_sizes.get(row['Source'], 0) * 2\n",
    "                buffer_end = pie_sizes.get(row['Destination'], 0) * 2\n",
    "\n",
    "                start_point = full_line.interpolate(buffer_start)\n",
    "                end_point = full_line.interpolate(full_line.length - buffer_end)\n",
    "\n",
    "                new_line = LineString([start_point, end_point])\n",
    "\n",
    "                value = row['Value'] / 7\n",
    "\n",
    "                arrow = patches.FancyArrowPatch((new_line.coords[0][0], new_line.coords[0][1]),\n",
    "                                                (new_line.coords[1][0], new_line.coords[1][1]),\n",
    "                                                mutation_scale=10,\n",
    "                                                color=arrow_color,\n",
    "                                                linewidth=value)\n",
    "                ax.add_patch(arrow)\n",
    "                arrow_patches.append(arrow)\n",
    "\n",
    "        # Draw pie charts\n",
    "        for zone, group in grouped_by_zone:\n",
    "            sizes = group['Value'].values\n",
    "            labels = group[type_col_labels].values\n",
    "\n",
    "            point = lat_long_info[lat_long_info['zone'] == zone]\n",
    "            if not point.empty:\n",
    "                lat, lon = point.iloc[0]['latitude'], point.iloc[0]['longitude']\n",
    "                if -125 <= lon <= -66.5 and 24.5 <= lat <= 49.5:\n",
    "                    size = pie_sizes.get(zone, 0)\n",
    "                    x, y = ax.transData.transform((lon, lat))\n",
    "                    pie_ax = inset_axes(ax, width=size, height=size, loc='center', bbox_to_anchor=(x, y))\n",
    "                    pie_colors = [color_mapping.get(label, 'gray') for label in labels]\n",
    "                    pie_ax.pie(sizes, autopct='', startangle=90, wedgeprops={'linewidth': 0.1, 'edgecolor': 'black'}, colors=pie_colors)\n",
    "                    pie_axes.append(pie_ax)\n",
    "\n",
    "        # Plot the solar terminator\n",
    "        terminator_coords = compute_terminator_coordinates(input_datetime)\n",
    "        if terminator_coords is not None and not terminator_coords.empty:\n",
    "            terminator_line, = ax.plot(terminator_coords['Longitude'], terminator_coords['Latitude'], color=\"orange\", linewidth=5, label=\"Solar Terminator\")\n",
    "        \n",
    "        \n",
    "        # Title\n",
    "        if chart_type == \"to\":\n",
    "            title = f\"Electricity Generation: {input_datetime.strftime('%m-%d %H:%M')}\"\n",
    "        else:\n",
    "            title = f\"Load: {input_datetime.strftime('%m-%d %H:%M')}\"\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Create legends\n",
    "    legend_GWs = [10, 50, 100]\n",
    "    legend_marker_sizes = [1/4 * GW for GW in legend_GWs]\n",
    "    pie_size_patches = [Line2D([0], [0], marker='o', color='grey', markerfacecolor='gray',\n",
    "                               markersize=legend_marker_sizes[i], label=f'{legend_GWs[i]} GW', linewidth=0)\n",
    "                        for i in range(len(legend_GWs))]\n",
    "    \n",
    "    pie_legend = ax.legend(handles=pie_size_patches, \n",
    "                           loc='lower left', \n",
    "                           fontsize=12, \n",
    "                           frameon=False,     \n",
    "                           handleheight=1, \n",
    "                           handlelength=2,\n",
    "                           labelspacing=1.5,\n",
    "    )\n",
    "\n",
    "    # Transmission line size legend\n",
    "    transmission_GWs = [10, 20, 30]  # Example GW values\n",
    "\n",
    "    transmission_patches = [\n",
    "    Line2D([0], [0], color=arrow_color, linewidth=transmission_GWs[i]/4, \n",
    "        label=f'{transmission_GWs[i]} GW') for i in range(len(transmission_GWs))\n",
    "    ]\n",
    "\n",
    "    transmission_legend = ax.legend(handles=transmission_patches, \n",
    "                            loc='lower left', \n",
    "                            fontsize=12, \n",
    "                            frameon=False, \n",
    "                            handleheight=1,\n",
    "                            handlelength=2,\n",
    "                            bbox_to_anchor=(0.15, 0.02))  # Adjust position\n",
    "    \n",
    "    source_legend_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=label)\n",
    "                              for label, color in color_mapping.items()]\n",
    "    source_legend = plt.legend(handles=source_legend_patches, loc='lower right', frameon=False)\n",
    "    ax.add_artist(pie_legend)\n",
    "    ax.add_artist(transmission_legend)  \n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Animate through all 24 hours\n",
    "    #frames = 24 * 4 - 3\n",
    "    frames = [52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92]\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=frames, interval=interval_ms, blit=False)\n",
    "\n",
    "    # Save animation\n",
    "    filename = f\"{save_path}/animation_{chart_type}_{date}.mp4\"\n",
    "    FFwriter = animation.FFMpegWriter(fps=frames_per_second)\n",
    "    anim.save(filename, writer=FFwriter)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Animation saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#energy_animation(date, shapefile_path, net_transmission, adp_from_interp, lat_long_info, color_mapping_elec_use, pie_multiplier, arrow_color, frames_per_second, save_path, \"from\", interpolate_minutes, scenario)\n",
    "energy_animation(date, shapefile_path, net_transmission, adp_to_interp, lat_long_info, color_mapping_to, pie_multiplier, arrow_color, frames_per_second, save_path, \"to\", interpolate_minutes, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animated_map_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
